{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import substratools as tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate dataset \n",
    "\n",
    "-> generate_data_samples.py\n",
    "\n",
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# train features, train labels, test features, test labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load into numpy array files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/fabien/python-projects/substra/substra/examples/mnist/data\n/home/fabien/python-projects/substra/substra/examples/mnist/assets\n"
    }
   ],
   "source": [
    "root_path = os.path.expanduser(\"~/python-projects/substra/substra/examples/mnist\")\n",
    "data_path = os.path.join(root_path, 'data')\n",
    "train_data_path = os.path.join(data_path, 'train')\n",
    "test_data_path = os.path.join(data_path, 'test')\n",
    "assets_path = os.path.join(root_path, 'assets')\n",
    "\n",
    "print(data_path)\n",
    "print(assets_path)\n",
    "\n",
    "OUT_FILE = {\n",
    "    os.path.join(\"train_data\",\"x_train.npy\"): x_train,\n",
    "    os.path.join(\"train_data\",\"y_train.npy\"): y_train,\n",
    "    os.path.join(\"test_data\", \"x_test.npy\"): x_test,\n",
    "    os.path.join(\"test_data\", \"y_test.npy\"): y_test,\n",
    "\n",
    "}\n",
    "\n",
    "for filename, data in OUT_FILE.items():\n",
    "    full_path = os.path.join(assets_path,filename)\n",
    "    os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
    "    np.save(full_path, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Dataset\n",
    "\n",
    "-> opener.py\n",
    "\n",
    "## Define fonctions of the Opener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MnistOpener(tools.Opener):\n",
    "\n",
    "    @classmethod\n",
    "    def _get_files(cls, folders):\n",
    "        \"\"\"Return list of X and y file given a folder location\"\"\"\n",
    "        X_files, y_files = [], []\n",
    "        for folder in folders:\n",
    "            Xs = glob.glob(os.path.join(folder, 'x*.npy'))\n",
    "            ys = glob.glob(os.path.join(folder, 'y*.npy'))\n",
    "\n",
    "            X_files.extend(Xs)\n",
    "            y_files.extend(ys)\n",
    "\n",
    "        return X_files, y_files\n",
    "\n",
    "    def get_X(self, folders):\n",
    "        \"\"\"Get X :-) \"\"\"\n",
    "        print('Finding features file...')\n",
    "        X_files, _ = self._get_files(folders)\n",
    "        print(X_files)\n",
    "        print('Loading features...')\n",
    "        Xs = []\n",
    "        for X_file in X_files:\n",
    "            Xs.append(np.load(X_file))\n",
    "        Xs = np.concatenate(Xs)\n",
    "\n",
    "        return Xs\n",
    "\n",
    "    def get_y(self, folders):\n",
    "        \"\"\"Get y :-)\"\"\"\n",
    "        print('Finding label file...')\n",
    "        _, y_files = self._get_files(folders)\n",
    "\n",
    "        print('Loading labels...')\n",
    "        ys = []\n",
    "        for y_file in y_files:\n",
    "            ys.append(np.load(y_file))\n",
    "        ys = np.concatenate(ys)\n",
    "\n",
    "        return ys\n",
    "\n",
    "    def fake_X(self):\n",
    "        return np.random.randn(22, 28, 28).astype(np.float32)\n",
    "\n",
    "    def fake_y(self):\n",
    "        return np.random.choice(np.arange(10), size=(22)).astype(np.int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset from local\n",
    "\n",
    "Replacing part of the command : \n",
    "```\n",
    "python assets/algo_random_forest/algo.py train \\\n",
    "  --debug \\\n",
    "  --opener-path assets/dataset/opener.py \\\n",
    "  --data-samples-path assets/train_data_samples \\\n",
    "  --output-model-path assets/model/model \\\n",
    "  --log-path assets/logs/train.log\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Finding features file...\n['/home/fabien/python-projects/substra/substra/examples/mnist/assets/train_data/x_train.npy']\nLoading features...\nx_train shape: (60000, 28, 28)\nFinding label file...\nLoading labels...\ny_train shape: (60000,)\nFinding features file...\n['/home/fabien/python-projects/substra/substra/examples/mnist/assets/test_data/x_test.npy']\nLoading features...\nx_test shape: (10000, 28, 28)\nFinding label file...\nLoading labels...\ny_test shape: (10000,)\n"
    }
   ],
   "source": [
    "\n",
    "folder_train_data = os.path.join(assets_path,\"train_data\")\n",
    "folder_test_data = os.path.join(assets_path,\"test_data\")\n",
    "\n",
    "folders = [folder_train_data]\n",
    "x_train = MnistOpener.get_X(MnistOpener, folders)\n",
    "print('x_train shape:', x_train.shape)\n",
    "y_train = MnistOpener.get_y(MnistOpener, folders)\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "folders = [folder_test_data]\n",
    "x_test = MnistOpener.get_X(MnistOpener, folders)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_test = MnistOpener.get_y(MnistOpener, folders)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm\n",
    "\n",
    "-> algo.py\n",
    "\n",
    "## Normalize data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "class Algo(tools.algo.Algo):\n",
    "\n",
    "    def _normalize_X(self, X):\n",
    "\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            X = X.reshape(X.shape[0], 1, img_rows, img_cols)\n",
    "       \n",
    "            input_shape = (1, img_rows, img_cols)\n",
    "        else:\n",
    "            X = X.reshape(X.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "            input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "        X = X.astype('float32')\n",
    "\n",
    "        X /= 255\n",
    "\n",
    "        print('X shape:', X.shape)\n",
    "        print(X.shape[0], ' samples')\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "X shape: (60000, 28, 28, 1)\n60000  samples\nX shape: (10000, 28, 28, 1)\n10000  samples\n"
    }
   ],
   "source": [
    "x_train = Algo._normalize_X(Algo,x_train)\n",
    "x_test = Algo._normalize_X(Algo,x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "-> function train(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 60000 samples, validate on 10000 samples\nEpoch 1/1\n60000/60000 [==============================] - 221s 4ms/step - loss: 0.2650 - accuracy: 0.9182 - val_loss: 0.0551 - val_accuracy: 0.9828\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x7f893fe7d5b0>"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 128\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model\n",
    "-> pas de fct associÃ©e dans l'exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Test loss: 0.05513126526260748\nTest accuracy: 0.9828000068664551\n"
    }
   ],
   "source": [
    "         \n",
    "score = model.evaluate(x_test, y_test, verbose=0) #split fct en 2 parts pour faire la metrics\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get model predictions\n",
    "\n",
    "-> fonction Algo.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, model):\n",
    "            y_pred = model.predict_classes(X, verbose=0)\n",
    "            return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(10000,)\n"
    }
   ],
   "source": [
    "y_pred = predict(x_test, model)\n",
    "\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load predictions\n",
    "( in opener.py )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(y_pred, path):\n",
    "    \"\"\"Save prediction\"\"\"\n",
    "    np.save(path, y_pred)\n",
    "\n",
    "def get_predictions(path):\n",
    "    \"\"\"Get predictions which were saved using the save_pred function\"\"\"\n",
    "    return np.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(10000,)\n"
    }
   ],
   "source": [
    "pred_path = os.path.join(assets_path,\"y_pred.npy\")\n",
    "save_predictions(y_pred, pred_path)\n",
    "\n",
    "y_pred = get_predictions(pred_path)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(self, path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return keras.models.load_model(f)\n",
    "\n",
    "def save_model(self, model, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, 'wb') as f:\n",
    "        model.save(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Test loss: 0.05513126526260748\nTest accuracy: 0.9828000068664551\n"
    }
   ],
   "source": [
    "folder_model = os.path.join(assets_path,\"model\")\n",
    "\n",
    "model_path = os.path.join(folder_model, \"model\")\n",
    "save_model(Algo, model, model_path)\n",
    "\n",
    "loaded_model = load_model(Algo, model_path)\n",
    "\n",
    "score = loaded_model.evaluate(x_test, y_test, verbose=0) #split fct en 2 parts pour faire la metrics\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "-> metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class MnistMetrics(tools.Metrics):\n",
    "    def score(self, y_true, y_pred):\n",
    "        \"\"\"Returns the macro-average recall\n",
    "\n",
    "        :param y_true: actual values from test data\n",
    "        :type y_true: pd.DataFrame\n",
    "        :param y_true: predicted values from test data\n",
    "        :type y_pred: pd.DataFrame\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return accuracy_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(10000, 10)\n(10000,)\n0.9828\n"
    }
   ],
   "source": [
    "print(y_test.shape)\n",
    "print(y_pred.shape)\n",
    "\n",
    "score = MnistMetrics.score(TitanicMetrics, np.argmax(y_test, axis=1), y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add dataset and objective to substra\n",
    "\n",
    "-> add_dataset_objective.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit807aa076f84d45f6af796336f3583d35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}